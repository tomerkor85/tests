<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ClickHouse Database - RadixInsight</title>
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="logo">
                <h1><a href="/">RadixInsight</a></h1>
                <p>Event-Based Analytics Platform</p>
            </div>
            <nav>
                <ul>
                    <li><a href="/#overview">Overview</a></li>
                    <li><a href="/#components">Components</a></li>
                    <li><a href="/#data-flow">Data Flow</a></li>
                    <li><a href="/#infrastructure">Infrastructure</a></li>
                    <li><a href="/#security">Security</a></li>
                    <li><a href="/#milestones">Milestones</a></li>
                    <li><a href="/#code">Code Samples</a></li>
                    <li><a href="/getting-started">Getting Started</a></li>
                    <li><a href="/api">API</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <section class="component-hero">
        <div class="container">
            <h1>ClickHouse Database</h1>
            <p>High-performance columnar database for event storage and analytics</p>
        </div>
    </section>

    <section class="component-details">
        <div class="container">
            <div class="component-section">
                <h2>Overview</h2>
                <p>ClickHouse serves as the primary storage engine for event data in the RadixInsight platform. As a high-performance columnar database, it is specifically designed for online analytical processing (OLAP) workloads, making it ideal for storing and analyzing the large volumes of event data generated by modern applications.</p>
                
                <p>Key advantages of ClickHouse for RadixInsight include:</p>
                <ul>
                    <li>Exceptional query performance for analytical workloads</li>
                    <li>Columnar storage format optimized for data compression</li>
                    <li>Linear scalability for handling growing data volumes</li>
                    <li>Support for real-time data ingestion and querying</li>
                    <li>Advanced aggregation and analytical functions</li>
                    <li>Efficient storage of time-series event data</li>
                    <li>Flexible schema design with support for nested structures</li>
                </ul>
            </div>

            <div class="component-section">
                <h2>Architecture Diagram</h2>
                <div class="architecture-diagram">
                    <img src="../images/clickhouse-architecture.png" alt="ClickHouse Architecture" onerror="this.onerror=null; this.src='data:image/svg+xml;charset=utf-8,%3Csvg xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22 width%3D%22800%22 height%3D%22400%22 viewBox%3D%220 0 800 400%22%3E%3Crect width%3D%22800%22 height%3D%22400%22 fill%3D%22%23f8f9fa%22%3E%3C%2Frect%3E%3Ctext x%3D%22400%22 y%3D%22200%22 font-family%3D%22Arial%2C sans-serif%22 font-size%3D%2224px%22 fill%3D%22%236c757d%22 text-anchor%3D%22middle%22 dominant-baseline%3D%22middle%22%3EClickHouse Architecture%3C%2Ftext%3E%3C%2Fsvg%3E';">
                    <div class="diagram-caption">
                        <p>The ClickHouse deployment for RadixInsight consists of:</p>
                        <ul>
                            <li><strong>ClickHouse Cluster</strong>: Multiple nodes for distributed storage and processing</li>
                            <li><strong>Sharding</strong>: Horizontal partitioning of data across nodes</li>
                            <li><strong>Replication</strong>: Data redundancy for fault tolerance</li>
                            <li><strong>Distributed Tables</strong>: Logical views that span physical shards</li>
                            <li><strong>ZooKeeper Ensemble</strong>: Coordination service for cluster management</li>
                            <li><strong>Load Balancer</strong>: Distributes client connections across nodes</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="component-section">
                <h2>Input/Output Specifications</h2>
                <div class="io-specs">
                    <div class="input-specs">
                        <h3>Inputs</h3>
                        <ul>
                            <li><strong>Event Data</strong>: Structured event records from the Event Consumer Service</li>
                            <li><strong>SQL Queries</strong>: Analytical queries from the Analytics Engine</li>
                            <li><strong>Schema Definitions</strong>: Table and view definitions</li>
                        </ul>
                    </div>
                    <div class="output-specs">
                        <h3>Outputs</h3>
                        <ul>
                            <li><strong>Query Results</strong>: Processed data for analytics operations</li>
                            <li><strong>Metadata</strong>: Schema information and statistics</li>
                            <li><strong>Performance Metrics</strong>: Query execution times and resource usage</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="component-section">
                <h2>Code Examples</h2>
                <div class="code-examples">
                    <h3>Schema Definition</h3>
                    <pre><code class="language-sql">-- events.sql
-- Main events table with sharding and replication

-- Create database if not exists
CREATE DATABASE IF NOT EXISTS radixinsight;

-- Create events table on each shard
CREATE TABLE IF NOT EXISTS radixinsight.events_local ON CLUSTER '{cluster}' (
    -- Event identification
    event_id String,
    event_name String,
    project_id String,
    
    -- Timestamps
    timestamp DateTime,
    server_timestamp DateTime,
    
    -- User identification
    user_id String,
    anonymous_id String,
    session_id String,
    
    -- Location data
    ip_address String,
    country String,
    region String,
    city String,
    
    -- Device and browser info
    browser String,
    os String,
    device String,
    
    -- Event properties (stored as JSON)
    properties String,
    
    -- Additional fields
    utm_source String,
    utm_medium String,
    utm_campaign String,
    
    -- Partition by day for efficient time-based queries
    _partition_date Date DEFAULT toDate(timestamp)
) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/radixinsight/events_local', '{replica}')
PARTITION BY toYYYYMM(_partition_date)
ORDER BY (project_id, timestamp, event_name, user_id)
TTL timestamp + INTERVAL 2 YEAR
SETTINGS index_granularity = 8192;

-- Create distributed table
CREATE TABLE IF NOT EXISTS radixinsight.events ON CLUSTER '{cluster}' AS radixinsight.events_local
ENGINE = Distributed('{cluster}', radixinsight, events_local, cityHash64(event_id));

-- Create materialized view for daily event counts
CREATE MATERIALIZED VIEW IF NOT EXISTS radixinsight.daily_event_counts ON CLUSTER '{cluster}'
ENGINE = SummingMergeTree
PARTITION BY toYYYYMM(date)
ORDER BY (project_id, date, event_name)
AS SELECT
    project_id,
    toDate(timestamp) AS date,
    event_name,
    count() AS count
FROM radixinsight.events_local
GROUP BY project_id, date, event_name;</code></pre>

                    <h3>ClickHouse Client Integration</h3>
                    <pre><code class="language-python"># clickhouse_client.py
import asyncio
import aiochclient
import aiohttp
from typing import Dict, List, Any, Optional
import logging
import json
from datetime import datetime, timedelta

from app.config import settings

logger = logging.getLogger(__name__)

class ClickHouseClient:
    """
    Client for interacting with ClickHouse database
    """
    def __init__(self):
        self.hosts = settings.CLICKHOUSE_HOSTS.split(',')
        self.user = settings.CLICKHOUSE_USER
        self.password = settings.CLICKHOUSE_PASSWORD
        self.database = settings.CLICKHOUSE_DATABASE
        self.session = None
        self.client = None
        
    async def connect(self):
        """
        Connect to ClickHouse
        """
        if self.client is not None:
            return
            
        self.session = aiohttp.ClientSession()
        
        # Use the first host for initial connection
        # Load balancing will be handled by the ClickHouse cluster
        host = self.hosts[0]
        
        self.client = aiochclient.ChClient(
            self.session,
            url=f"http://{host}:8123",
            user=self.user,
            password=self.password,
            database=self.database
        )
        
        logger.info(f"Connected to ClickHouse at {host}")
        
    async def close(self):
        """
        Close connection to ClickHouse
        """
        if self.session is not None:
            await self.session.close()
            self.session = None
            self.client = None
            logger.info("Closed ClickHouse connection")
            
    async def execute(self, query: str, params: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        Execute a query against ClickHouse
        """
        if self.client is None:
            await self.connect()
            
        try:
            start_time = datetime.now()
            result = [row async for row in self.client.execute(query, params)]
            execution_time = (datetime.now() - start_time).total_seconds()
            
            logger.debug(f"Query executed in {execution_time:.3f}s: {query[:100]}...")
            return result
        except Exception as e:
            logger.error(f"Error executing query: {str(e)}")
            logger.error(f"Query: {query}")
            raise
            
    async def insert_events(self, events: List[Dict[str, Any]]):
        """
        Insert events into ClickHouse
        """
        if not events:
            return
            
        if self.client is None:
            await self.connect()
            
        # Prepare events for insertion
        prepared_events = []
        for event in events:
            # Extract required fields
            prepared_event = {
                "event_id": event.get("event_id", ""),
                "event_name": event.get("event_name", ""),
                "project_id": event.get("project_id", ""),
                "timestamp": event.get("timestamp", 0),
                "server_timestamp": event.get("server_timestamp", 0),
                "user_id": event.get("user_id", ""),
                "anonymous_id": event.get("anonymous_id", ""),
                "session_id": event.get("session_id", ""),
                "ip_address": event.get("ip_address", ""),
                
                # Extract geo data
                "country": event.get("geo", {}).get("country", ""),
                "region": event.get("geo", {}).get("region", ""),
                "city": event.get("geo", {}).get("city", ""),
                
                # Extract device info
                "browser": event.get("user_agent", {}).get("browser", ""),
                "os": event.get("user_agent", {}).get("os", ""),
                "device": event.get("user_agent", {}).get("device", ""),
                
                # Serialize properties as JSON
                "properties": json.dumps(event.get("properties", {})),
                
                # Extract UTM parameters
                "utm_source": event.get("utm", {}).get("utm_source", ""),
                "utm_medium": event.get("utm", {}).get("utm_medium", ""),
                "utm_campaign": event.get("utm", {}).get("utm_campaign", "")
            }
            
            prepared_events.append(prepared_event)
            
        # Insert into ClickHouse
        query = f"INSERT INTO {self.database}.events"
        await self.client.execute(query, *prepared_events)
        
        logger.info(f"Inserted {len(events)} events into ClickHouse")
            
    async def get_event_count(self, project_id: str, start_date: str, end_date: str, event_name: Optional[str] = None) -> int:
        """
        Get count of events for a project in a date range
        """
        query = f"""
        SELECT count() as count
        FROM {self.database}.events
        WHERE project_id = '{project_id}'
          AND timestamp >= toDateTime('{start_date}')
          AND timestamp < toDateTime('{end_date}')
        """
        
        if event_name:
            query += f" AND event_name = '{event_name}'"
            
        result = await self.execute(query)
        return result[0]["count"] if result else 0</code></pre>

                    <h3>Query Optimization Example</h3>
                    <pre><code class="language-sql">-- Example of optimized ClickHouse query for user retention analysis

-- Calculate daily retention for users who performed a specific event
WITH first_events AS (
    -- Get first occurrence of the event for each user
    SELECT
        user_id,
        min(toDate(timestamp)) AS first_date
    FROM radixinsight.events
    WHERE project_id = 'project123'
      AND event_name = 'signup'
      AND timestamp >= toDateTime('2025-01-01')
      AND timestamp < toDateTime('2025-04-01')
      AND user_id != ''
    GROUP BY user_id
),
daily_activity AS (
    -- Get daily activity for all users
    SELECT
        user_id,
        toDate(timestamp) AS activity_date
    FROM radixinsight.events
    WHERE project_id = 'project123'
      AND timestamp >= toDateTime('2025-01-01')
      AND timestamp < toDateTime('2025-04-01')
      AND user_id != ''
    GROUP BY user_id, activity_date
)
-- Calculate retention by day
SELECT
    first_date,
    dateDiff('day', first_date, activity_date) AS day_number,
    count(DISTINCT fe.user_id) AS total_users,
    count(DISTINCT da.user_id) AS active_users,
    round(count(DISTINCT da.user_id) / count(DISTINCT fe.user_id) * 100, 2) AS retention_rate
FROM first_events fe
LEFT JOIN daily_activity da ON fe.user_id = da.user_id AND activity_date >= first_date
GROUP BY first_date, day_number
HAVING day_number >= 0 AND day_number <= 30
ORDER BY first_date, day_number;</code></pre>
                </div>
            </div>

            <div class="component-section">
                <h2>API Integration</h2>
                <p>ClickHouse does not expose external API endpoints directly in the RadixInsight architecture. Instead, it is accessed through the following components:</p>
                <ul>
                    <li><a href="/event-consumer-service">Event Consumer Service</a> - Writes event data to ClickHouse</li>
                    <li><a href="/analytics-engine">Analytics Engine</a> - Reads and queries data from ClickHouse</li>
                </ul>
                <p>For API endpoints that indirectly access ClickHouse data, visit the <a href="/api">RadixInsight API Reference</a>.</p>
            </div>

            <div class="component-section">
                <h2>Component Relationships</h2>
                <div class="component-relationships">
                    <h3>Dependencies</h3>
                    <p>ClickHouse depends on the following components:</p>
                    <ul>
                        <li>ZooKeeper - For cluster coordination and metadata management</li>
                        <li>GCP Persistent Disks - For data storage</li>
                    </ul>


</div>
            </div>
        </div>
    </section>
</body>
